#import "@preview/codly:1.3.0": codly-init

#show: codly-init
#show link: set text(fill: blue)
#set text(font: "Source Sans 3")
#set heading(numbering: "1.1")

#align(
  center,
  text(size: 2em, weight: "bold")[Research report]
)

#align(
  center,
  text(size: 1.5em, weight: "bold")[Solutions for texture projection]
)

#align(
  center,
  text(size: 1em)[Author - Jeremy Duc]
)

= Foreword

The goal of the research phase is to explore potential solutions for the texture projection problem : a way to project a texture on a 3D model, given a 2D image of the texture and a 3D model of the object and then get the UV coordinates of the projection.

Mutliple scenarios are considered :
+ The LLM with input(.obj and prompt describing the texture) outputs complete solution (e.g. UV coordinates + texture)
+ The LLM outputs an intermediate representation (e.g. UV coordinates) and a separate module is responsible for applying the texture on the 3D model
+ The LLM only outputs sides views of the texture and a separate module is responsible for projecting it on the 3D model

= Research
+ Online research showed that solutions to scenario 1 exists since few years and are commercially available. For example, Scenario's Texture#footnote(link("https://www.scenario.com/blog/ai-texture-generation")) is a web-app that can generate textures from images and apply them to 3D models. However, those web solutions are not open source nor free to use. Thus, researchs have been oriented toward a local solution that can be integrated in our pipeline is necessary.
+ A publication : TEXTure: Text-Guided Texturing of 3D Shapes#footnote(link("https://texturepaper.github.io/TEXTurePaper/")) provide potential solution. The paper describes an iterative framework leveraging a pretrained depth-to-image diffusion model to synthesize, edit, and transfer high-fidelity 3D textures. This finding is promising and has been explored. 
- An local analysis of the forked repo showed that possible outputs are fitting our need : 
  -  1. `mesh.obj` : contains the 3D model of the object with the UV coordinates corresponding to the texture projection
   2. `albedo.png` : The texture image, generated by AI.
   3. `mesh.obj` (sections `vt`) : Contains the UV coordinates of the projection, which can be used to apply the texture on the 3D model.
   4. `mesh.mtl` : The material file that references the texture and can be used to apply it on the 3D model.
- Next step is to test the repo in contained environment (due to librairies issues) and see if the outputs desired are possible to retrieve.
  





= Resources Consulted

To stucture our approach to UV projection and mesh parameterization, the following works and discussions were analyzed:

=== 1. Theory & Fundamentals
- #link("https://en.wikipedia.org/wiki/UV_mapping")[UV Mapping and Mesh Parameterization Concepts (Wikipedia)]

=== 2. Algorithmic Implementation
- #link("https://stackoverflow.com/questions/4083944/opengl-texture-mapping?rq=3")[OpenGL Texture Mapping Architecture and Workflows (StackOverflow)]
- #link("https://stackoverflow.com/questions/14440455/uv-texture-mapping-in-python")[Approaches to UV Texture Mapping Implementations in Python (StackOverflow)]
- #link("https://stackoverflow.com/questions/14878467/how-to-generate-texture-mapping-images")[Algorithmic Generation of Texture Mapping Images (StackOverflow)]

=== 3. Libraries & Tooling
- #link("https://github.com/jpcy/xatlas")[xatlas: Open-source Mesh Parameterization and UV Unwrapping Library (GitHub)]